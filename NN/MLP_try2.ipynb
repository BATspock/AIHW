{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64babc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from numpy import genfromtxt\n",
    "\n",
    "#start=datetime.now()\n",
    "\n",
    "\n",
    "n = len(sys.argv)\n",
    "if n==4:\n",
    "    x_train = sys.argv[1]\n",
    "    y_train = sys.argv[2]\n",
    "    x_test = sys.argv[3]\n",
    "    x_train = genfromtxt(x_train, delimiter=',')\n",
    "    x_test = genfromtxt(x_test, delimiter=',')\n",
    "    y_train = genfromtxt(y_train, delimiter=',')\n",
    "else:\n",
    "    x_train = genfromtxt('/home/adkishor/projects/misc/CSCI561-AI/mlp/publicdata/train_image.csv', delimiter=',')\n",
    "    x_test = genfromtxt('/home/adkishor/projects/misc/CSCI561-AI/mlp/publicdata/test_image.csv', delimiter=',')\n",
    "    y_train = genfromtxt('/home/adkishor/projects/misc/CSCI561-AI/mlp/publicdata/train_label.csv', delimiter=',')\n",
    "    \n",
    "print(x_train.shape, x_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10436d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (784, 10000) (10, 60000)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "digits = 10\n",
    "\n",
    "m = x_train.shape[0]\n",
    "m_test = x_test.shape[0]\n",
    "\n",
    "y_train = y_train.reshape(1,m)\n",
    "#y_test = y_test.reshape(1,m_test)\n",
    "\n",
    "y_new_train = np.eye(digits)[y_train.astype('int32')]\n",
    "y_new_train = y_new_train.T.reshape(10,m)\n",
    "\n",
    "\n",
    "#y_new_test = np.eye(digits)[y_test.astype('int32')]\n",
    "#y_new_test = y_new_test.T.reshape(10,m_test)\n",
    "\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_new_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5803e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken = 0:01:38.422952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#getting the test and training set ready\n",
    "\n",
    "#x_train = genfromtxt('train_image.csv', delimiter=',')\n",
    "#x_test = genfromtxt('test_image.csv', delimiter=',')\n",
    "#y_train = genfromtxt('train_label.csv', delimiter=',')\n",
    "#y_test = genfromtxt('test_label.csv', delimiter=',')\n",
    "\n",
    "#x_test = genfromtxt('train_image.csv', delimiter=',')\n",
    "#x_train = genfromtxt('test_image.csv', delimiter=',')\n",
    "#y_test = genfromtxt('train_label.csv', delimiter=',')\n",
    "#y_train = genfromtxt('test_label.csv', delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "#cross entropy loss\n",
    "def compute_loss(X, Y):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(X, np.log(Y)))\n",
    "    m = X.shape[1]\n",
    "    L = -(1/m) * L_sum\n",
    "\n",
    "    return L\n",
    "\n",
    "def softmax(z):\n",
    "    s = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    return s\n",
    "\n",
    "#forward pass\n",
    "def feed_forward(X, parameters):\n",
    "\n",
    "    cache = {}\n",
    "    #Z1 = W1*X+b1\n",
    "    cache[\"Z1\"] = np.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
    "    #A1 = sigmoid(Z1)\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    #Z2 = W2*A1+b2\n",
    "    cache[\"Z2\"] = np.matmul(parameters[\"W2\"], cache[\"A1\"]) + parameters[\"b2\"]\n",
    "    #A2 = sigmoid(Z2)\n",
    "    cache[\"A2\"] = sigmoid(cache[\"Z2\"])\n",
    "    #Z3 = W3*A2+b3\n",
    "    cache[\"Z3\"] = np.matmul(parameters[\"W3\"], cache[\"A2\"]) + parameters[\"b3\"]\n",
    "    #A3 = softmax(Z3)\n",
    "    cache[\"A3\"] = softmax(cache[\"Z3\"])\n",
    "\n",
    "    return cache\n",
    "\n",
    "def back_propagate(X, Y, parameters, cache):\n",
    "    #find differentiation of all the parameters and update the gradients and store in the dictionary\n",
    "\n",
    "    #softmax cost differentiation eventually boils down to difference of the terms \n",
    "    dZ3 = cache[\"A3\"] - Y\n",
    "    dW3 = (1./m_batch) * np.matmul(dZ3, cache[\"A2\"].T)\n",
    "    db3 = (1./m_batch) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.matmul(parameters[\"W3\"].T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid(cache[\"Z2\"]) * (1 - sigmoid(cache[\"Z2\"]))\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(parameters[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    gradient = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}\n",
    "\n",
    "    return gradient\n",
    "\n",
    "np.random.seed(138)\n",
    "\n",
    "\n",
    "X = x_train\n",
    "Y = y_new_train\n",
    "\n",
    "#hyperparameters\n",
    "n_x = x_train.shape[0]\n",
    "n_h1 = 512\n",
    "n_h2 = 64\n",
    "learning_rate = 4\n",
    "beta = .9\n",
    "batch_size = 64\n",
    "batches = -(-m // batch_size)\n",
    "epochs = 20\n",
    "\n",
    "# initialization\n",
    "parameters = { \"W1\": np.random.randn(n_h1, n_x) * np.sqrt(1. / n_x),\n",
    "           \"b1\": np.zeros((n_h1, 1)) * np.sqrt(1. / n_x),\n",
    "           \"W2\": np.random.randn(n_h2, n_h1) * np.sqrt(1. / n_h1),\n",
    "           \"b2\": np.zeros((n_h2, 1)) * np.sqrt(1. / n_h1),\n",
    "           \"W3\": np.random.randn(digits, n_h2) * np.sqrt(1. / n_h2),\n",
    "           \"b3\": np.zeros((digits, 1)) * np.sqrt(1. / n_h2)}\n",
    "\n",
    "V_dW1 = np.zeros(parameters[\"W1\"].shape)\n",
    "V_db1 = np.zeros(parameters[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(parameters[\"W2\"].shape)\n",
    "V_db2 = np.zeros(parameters[\"b2\"].shape)\n",
    "V_dW3 = np.zeros(parameters[\"W3\"].shape)\n",
    "V_db3 = np.zeros(parameters[\"b3\"].shape)\n",
    "\n",
    "\n",
    "# training the model\n",
    "for i in range(epochs):\n",
    "\n",
    "    #using mini batch gradient descent\n",
    "    permutation = np.random.permutation(x_train.shape[1])\n",
    "    X_train_shuffled = x_train[:, permutation]\n",
    "    Y_train_shuffled = y_new_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, x_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        cache = feed_forward(X, parameters)\n",
    "        gradient = back_propagate(X, Y, parameters, cache)\n",
    "\n",
    "        V_dW1 = (beta * V_dW1 + (1. - beta) * gradient[\"dW1\"])\n",
    "        V_db1 = (beta * V_db1 + (1. - beta) * gradient[\"db1\"])\n",
    "        V_dW2 = (beta * V_dW2 + (1. - beta) * gradient[\"dW2\"])\n",
    "        V_db2 = (beta * V_db2 + (1. - beta) * gradient[\"db2\"])\n",
    "        V_dW3 = (beta * V_dW3 + (1. - beta) * gradient[\"dW3\"])\n",
    "        V_db3 = (beta * V_db3 + (1. - beta) * gradient[\"db3\"])\n",
    "\n",
    "        parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * V_dW1\n",
    "        parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * V_db1\n",
    "        parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * V_dW2\n",
    "        parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * V_db2\n",
    "        parameters[\"W3\"] = parameters[\"W3\"] - learning_rate * V_dW3\n",
    "        parameters[\"b3\"] = parameters[\"b3\"] - learning_rate * V_db3\n",
    "\n",
    "    cache = feed_forward(x_train, parameters)\n",
    "    train_cost = compute_loss(y_new_train, cache[\"A3\"])\n",
    "    #print(\"Epoch {}: training cost = {}\".format(i+1 ,train_cost))\n",
    "\n",
    "#print(\"Done.\")\n",
    "\n",
    "#prediction\n",
    "cache = feed_forward(x_test, parameters)\n",
    "predictions = np.argmax(cache[\"A3\"], axis=0)\n",
    "\n",
    "#labels = np.argmax(y_new_test, axis=0)\n",
    "\n",
    "#print(classification_report(predictions, labels))\n",
    "\n",
    "#print(confusion_matrix(predictions, labels))\n",
    "#print(classification_report(predictions, labels))\n",
    "\n",
    "#correct = 0\n",
    "#for i in range(len(predictions)):\n",
    "#    if predictions[i] == labels[i]:\n",
    "#        correct = correct + 1\n",
    "#print(correct)\n",
    "\n",
    "#saving predictions to test_predictions.csv\n",
    "np.savetxt(\"test_predictions.csv\", predictions, delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "# endtime = datetime.now()\n",
    "# print('time taken =', endtime - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28f129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
